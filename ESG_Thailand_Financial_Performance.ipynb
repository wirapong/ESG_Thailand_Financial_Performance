{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNJigoslpnZraUNV4ZJ4kL1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wirapong/ESG_Thailand_Financial_Performance/blob/main/ESG_Thailand_Financial_Performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ESG → Financial Performance — Replication Template\n",
        "# Paper: \"The Impact of ESG Performance on Financial Performance: Evidence from Listed Companies in Thailand\"\n",
        "# Requirements: pandas, numpy, statsmodels, scipy, tabulate (optional)\n",
        "# pip install pandas numpy statsmodels scipy tabulate\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n"
      ],
      "metadata": {
        "id": "4vVMFohCjpSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import statsmodels.api as sm\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "33YdZfoglIfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 0) INPUTS\n",
        "# -----------------------------------------------------------------------------\n",
        "# Expected CSV schema (wide/panel, one row per firm-year):\n",
        "# firm_id, year, EBIT, CapitalEmployed, NetIncome, TotalAssets, ES, SS, GS, ESG, SIZE, LEVERAGE, ROCE, ROA\n",
        "# If ROCE/ROA are missing, they will be computed from EBIT/CapitalEmployed and NetIncome/TotalAssets.\n",
        "\n",
        "CSV_PATH = \"Thailand_ESG__data_30102025.csv\"  # <-- change to your file path\n",
        "FIRM_COL = \"firm_id\"\n",
        "TIME_COL = \"year\"\n",
        "\n",
        "DEPENDENT_VARS = [\"ROCE\", \"ROA\"]\n",
        "#ESG_INDEP_VARS = [\"ES\", \"SS\", \"GS\", \"ESG\"]  # article uses both pillar scores and total ESG\n",
        "ESG_INDEP_VARS = [\"ESG\"]  # article uses both pillar scores and total ESG\n",
        "CONTROL_VARS = [\"SIZE\", \"LEVERAGE\"]"
      ],
      "metadata": {
        "id": "UumP4v1mlFE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# 1) LOAD & PREP\n",
        "# -----------------------------------------------------------------------------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# Ensure numeric\n",
        "for col in [\"EBIT\", \"CapitalEmployed\", \"NetIncome\", \"TotalAssets\"] + ESG_INDEP_VARS + CONTROL_VARS + [\"ROCE\", \"ROA\"]:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "# Compute ROCE, ROA if missing (article definitions)\n",
        "if \"ROCE\" not in df.columns and {\"EBIT\", \"CapitalEmployed\"}.issubset(df.columns):\n",
        "    df[\"ROCE\"] = df[\"EBIT\"] / df[\"CapitalEmployed\"]\n",
        "\n",
        "if \"ROA\" not in df.columns and {\"NetIncome\", \"TotalAssets\"}.issubset(df.columns):\n",
        "    df[\"ROA\"] = df[\"NetIncome\"] / df[\"TotalAssets\"]\n",
        "\n",
        "# Drop rows with critical missing values\n",
        "needed = list(set(DEPENDENT_VARS + ESG_INDEP_VARS + CONTROL_VARS))\n",
        "df = df.dropna(subset=needed)\n",
        "\n",
        "# Optional: clip/guard against division issues/outliers\n",
        "df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=needed)"
      ],
      "metadata": {
        "id": "Op3POTk-mmTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# 2) DESCRIPTIVE STATISTICS (Table 3 analogue)\n",
        "# -----------------------------------------------------------------------------\n",
        "def descriptive_table(frame, cols):\n",
        "    out = []\n",
        "    for c in cols:\n",
        "        s = frame[c].dropna()\n",
        "        out.append({\n",
        "            \"Variable\": c,\n",
        "            \"Range\": s.max() - s.min(),\n",
        "            \"Min\": s.min(),\n",
        "            \"Max\": s.max(),\n",
        "            \"Mean\": s.mean(),\n",
        "            \"Std\": s.std(ddof=1)\n",
        "        })\n",
        "    return pd.DataFrame(out)\n",
        "\n",
        "#desc_cols = [\"ROCE\", \"ROA\", \"ES\", \"SS\", \"GS\", \"ESG\", \"SIZE\", \"LEVERAGE\"]\n",
        "desc_cols = [\"ROCE\", \"ROA\", \"ESG\", \"SIZE\", \"LEVERAGE\"]\n",
        "desc = descriptive_table(df, [c for c in desc_cols if c in df.columns])\n",
        "print(\"\\n=== Descriptive Statistics (like Table 3) ===\")\n",
        "print(desc.to_string(index=False))\n",
        "\n"
      ],
      "metadata": {
        "id": "RGfbnpFkmpaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# 3) CORRELATION MATRIX & MULTICOLLINEARITY (VIF) (Table 5 analogue)\n",
        "# -----------------------------------------------------------------------------\n",
        "#corr_vars = [\"ROCE\", \"ROA\", \"ES\", \"SS\", \"GS\", \"ESG\", \"SIZE\", \"LEVERAGE\"]\n",
        "corr_vars = [\"ROCE\", \"ROA\", \"ESG\", \"SIZE\", \"LEVERAGE\"]\n",
        "corr_vars = [c for c in corr_vars if c in df.columns]\n",
        "corr = df[corr_vars].corr()\n",
        "print(\"\\n=== Correlation Matrix (like Table 5) ===\")\n",
        "print(corr.round(3).to_string())\n",
        "\n",
        "# VIF on predictors only (as in article, check threshold ~10)\n",
        "X_for_vif = df[ESG_INDEP_VARS + CONTROL_VARS].dropna()\n",
        "X_for_vif_const = sm.add_constant(X_for_vif)\n",
        "vif_list = []\n",
        "for i, col in enumerate(X_for_vif_const.columns):\n",
        "    if col == \"const\":  # skip intercept\n",
        "        continue\n",
        "    vif_val = variance_inflation_factor(X_for_vif_const.values, i)\n",
        "    tol = 1.0 / vif_val if vif_val != 0 else np.nan\n",
        "    vif_list.append({\"Variable\": col, \"VIF\": vif_val, \"Tolerance\": tol})\n",
        "vif_df = pd.DataFrame(vif_list)\n",
        "print(\"\\n=== Multicollinearity Test (VIF & Tolerance) ===\")\n",
        "print(vif_df.round(3).to_string(index=False))"
      ],
      "metadata": {
        "id": "hzWv2zAbmxWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# 4) UNIT ROOT (ADF) TESTS (Table 4 analogue)\n",
        "# Note: The paper applies ADF to make series stationary. With short panels (e.g., T=3),\n",
        "# ADF is not very informative; still, we show pooled-series ADF as an approximation.\n",
        "# For better panel stationarity testing, consider panel unit root tests (e.g., IPS/LLC).\n",
        "# -----------------------------------------------------------------------------\n",
        "def pooled_series(frame, col, firm_col=FIRM_COL, time_col=TIME_COL):\n",
        "    # Stack all firm time series end-to-end in time order for a rough pooled ADF\n",
        "    tmp = frame[[firm_col, time_col, col]].dropna().sort_values([firm_col, time_col])\n",
        "    return tmp[col].astype(float).values\n",
        "\n",
        "print(\"\\n=== Augmented Dickey–Fuller (ADF) — pooled approximation ===\")\n",
        "adf_rows = []\n",
        "#for col in [\"ES\",\"SS\",\"GS\",\"ESG\",\"SIZE\",\"LEVERAGE\",\"ROCE\",\"ROA\"]:\n",
        "for col in [\"ESG\",\"SIZE\",\"LEVERAGE\",\"ROCE\",\"ROA\"]:\n",
        "    if col in df.columns:\n",
        "        series = pooled_series(df, col)\n",
        "        if len(series) > 10:  # ADF needs reasonable length\n",
        "            adf_stat, pval, _, _, crit, _ = adfuller(series, autolag=\"AIC\")\n",
        "            adf_rows.append({\n",
        "                \"Variable\": col,\n",
        "                \"ADF t-stat\": adf_stat,\n",
        "                \"p-value\": pval\n",
        "            })\n",
        "        else:\n",
        "            adf_rows.append({\"Variable\": col, \"ADF t-stat\": np.nan, \"p-value\": np.nan})\n",
        "adf_df = pd.DataFrame(adf_rows)\n",
        "print(adf_df.round(4).to_string(index=False))"
      ],
      "metadata": {
        "id": "Qj9xJdhOm1Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# 5) GRANGER CAUSALITY TESTS (Table 6 analogue)\n",
        "# The article tests whether ESG components \"Granger-cause\" financial metrics.\n",
        "# We'll run tests for (ES, SS, GS, ESG, SIZE, LEVERAGE) → (ROCE, ROA)\n",
        "# by pooling series (approximation). Choose maxlag=2 as a reasonable default.\n",
        "# -----------------------------------------------------------------------------\n",
        "def granger_pooled_causality(frame, cause, effect, maxlag=2):\n",
        "    \"\"\"\n",
        "    Rough pooled Granger: concatenate firm series; run on 2-column DataFrame [effect, cause]\n",
        "    Returns min p-value across lags for F-test of Granger causality.\n",
        "    \"\"\"\n",
        "    cols = [cause, effect, FIRM_COL, TIME_COL]\n",
        "    gdf = frame[cols].dropna().sort_values([FIRM_COL, TIME_COL]).reset_index(drop=True)\n",
        "    # build 2-col data in correct order for statsmodels: [effect, cause]\n",
        "    data = gdf[[effect, cause]].astype(float)\n",
        "    try:\n",
        "        res = grangercausalitytests(data, maxlag=maxlag, verbose=False)\n",
        "        # collect p-values for ssr_ftest across lags\n",
        "        pvals = [res[lag][0][\"ssr_ftest\"][1] for lag in range(1, maxlag+1)]\n",
        "        return float(np.nanmin(pvals)), pvals\n",
        "    except Exception as e:\n",
        "        return np.nan, []\n",
        "\n",
        "print(\"\\n=== Granger Causality (pooled approximation; p-values) ===\")\n",
        "#causes = [c for c in [\"ES\",\"SS\",\"GS\",\"ESG\",\"SIZE\",\"LEVERAGE\"] if c in df.columns]\n",
        "causes = [c for c in [\"ESG\",\"SIZE\",\"LEVERAGE\"] if c in df.columns]\n",
        "effects = [c for c in [\"ROCE\",\"ROA\"] if c in df.columns]\n",
        "granger_results = []\n",
        "for eff in effects:\n",
        "    for cau in causes:\n",
        "        pmin, all_p = granger_pooled_causality(df, cause=cau, effect=eff, maxlag=2)\n",
        "        granger_results.append({\"Cause → Effect\": f\"{cau} → {eff}\", \"min p-val (lags≤2)\": pmin, \"all p-vals\": all_p})\n",
        "granger_df = pd.DataFrame(granger_results)\n",
        "print(granger_df.to_string(index=False))"
      ],
      "metadata": {
        "id": "VvvdQF43nDFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# 6) MULTIPLE REGRESSION — POOLED OLS (Models 1 & 2)\n",
        "# Model 1: ROCE ~ ES + SS + GS + ESG + SIZE + LEVERAGE\n",
        "# Model 2: ROA  ~ ES + SS + GS + ESG + SIZE + LEVERAGE\n",
        "# -----------------------------------------------------------------------------\n",
        "def pooled_ols(frame, y, Xcols):\n",
        "    X = frame[Xcols].copy()\n",
        "    yv = frame[y].astype(float)\n",
        "    X = sm.add_constant(X.astype(float))\n",
        "    model = sm.OLS(yv, X)\n",
        "    fit = model.fit()  # you may use .fit(cov_type=\"HC3\") for robust SEs\n",
        "    # Durbin–Watson\n",
        "    dw = sm.stats.stattools.durbin_watson(fit.resid)\n",
        "    return fit, dw\n",
        "\n",
        "def print_regression(fit, dw):\n",
        "    print(f\"\\nR-squared: {fit.rsquared:.6f} | Adj R-squared: {fit.rsquared_adj:.6f}\")\n",
        "    print(f\"F-statistic: {fit.fvalue:.6f} | Prob(F): {fit.f_pvalue:.6g}\")\n",
        "    print(f\"Durbin–Watson: {dw:.6f}\")\n",
        "    print(\"\\nCoefficients:\")\n",
        "    coefs = pd.DataFrame({\n",
        "        \"coef\": fit.params,\n",
        "        \"std_err\": fit.bse,\n",
        "        \"t\": fit.tvalues,\n",
        "        \"p>|t|\": fit.pvalues\n",
        "    })\n",
        "    print(coefs.round(6).to_string())\n",
        "\n",
        "model_vars = [c for c in (ESG_INDEP_VARS + CONTROL_VARS) if c in df.columns]\n",
        "\n",
        "if \"ROCE\" in df.columns:\n",
        "    #print(\"\\n=== Model 1: ROCE ~ ES + SS + GS + ESG + SIZE + LEVERAGE ===\")\n",
        "    print(\"\\n=== Model 1: ROCE ~ ESG + SIZE + LEVERAGE ===\")\n",
        "    fit1, dw1 = pooled_ols(df, \"ROCE\", model_vars)\n",
        "    print_regression(fit1, dw1)\n",
        "\n",
        "if \"ROA\" in df.columns:\n",
        "    #print(\"\\n=== Model 2: ROA  ~ ES + SS + GS + ESG + SIZE + LEVERAGE ===\")\n",
        "    print(\"\\n=== Model 2: ROA  ~ ESG + SIZE + LEVERAGE ===\")\n",
        "    fit2, dw2 = pooled_ols(df, \"ROA\", model_vars)\n",
        "    print_regression(fit2, dw2)\n"
      ],
      "metadata": {
        "id": "-5yZzc8znHDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# 7) OPTIONAL — COEFFICIENT-FORM EQUATIONS (to mirror the paper’s presentation)\n",
        "# -----------------------------------------------------------------------------\n",
        "def equation_text(fit, yname):\n",
        "    terms = [f\"{coef:+.6f}*{name}\" for name, coef in fit.params.items() if name != \"const\"]\n",
        "    return f\"{yname} = {fit.params['const']:+.6f} \" + \" \".join(terms)\n",
        "\n",
        "if \"ROCE\" in df.columns:\n",
        "    print(\"\\nEquation (Model 1):\")\n",
        "    print(equation_text(fit1, \"ROCE\"))\n",
        "\n",
        "if \"ROA\" in df.columns:\n",
        "    print(\"\\nEquation (Model 2):\")\n",
        "    print(equation_text(fit2, \"ROA\"))"
      ],
      "metadata": {
        "id": "jBjAwvUtnLHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# 8) NOTES FOR REPLICATION FIDELITY\n",
        "# -----------------------------------------------------------------------------\n",
        "# • This script treats the data as pooled OLS, matching the article’s multiple regression setup.\n",
        "# • The ADF and Granger sections implement pooled/concatenated approximations; with T=3 per firm,\n",
        "#   time-series power is limited. For high-fidelity time-series inference, use longer panels or\n",
        "#   panel-time-series methods (e.g., panel unit root / panel Granger).\n",
        "# • Ensure ES, SS, GS, and ESG match your source methodology (e.g., CRISIL weights: E=35%, S=25%, G=40%).\n",
        "# • SIZE (Total Assets) and LEVERAGE should follow the same definitions used when compiling your dataset."
      ],
      "metadata": {
        "id": "SVDEARJDmfSQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}